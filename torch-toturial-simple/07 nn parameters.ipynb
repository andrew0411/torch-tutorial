{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수 (Loss Function)\n",
    "- 예측 값과 실제 값 사이의 오차를 측정\n",
    "\n",
    "- 학습이 진행되면서 해당 학습 과정이 얼마나 잘 진행되고 있는지 나타내는 지표\n",
    "\n",
    "- 모델이 훈련되는 동안 최소화 시켜야할 값으로 주어진 문제에 대한 성공 지표\n",
    "\n",
    "- 손실 함수에 따른 결과를 통해 학습 파라미터를 조정\n",
    "\n",
    "- 최적화 이론에서 최소화하고자 하는 함수\n",
    "\n",
    "- 미분 가능한 함수 (실제로 convex할 때 optima가 존재하고, 찾을 수 있기 때문)\n",
    "\n",
    "- torch의 주요 loss function\n",
    "    - `torch.nn.BCELoss` : 이진 분류를 위해 사용\n",
    "\n",
    "    - `torch.nn.CrossEntropyLoss` : 다중 클래스 분류를 위해 사용\n",
    "\n",
    "    - `torch.nn.MSELoss` : 회귀 모델에서 사용 \n",
    "\n",
    "    - `torch.nn.L1Loss` : 다른 loss와 더해져서 regularization 효과를 줄 때 사용\n",
    "\n",
    "    - `torch.nn.KLDivLoss` : KL-Divergence를 계산해줌 (어떤 두 분포의 차이를 볼 때 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- 손실 함수를 기반으로 모델이 어떻게 update 되어야하는지 결정 (Gradient Descent를 수행하는 특정 알고리즘)\n",
    "\n",
    "- optimizer는 `step()`을 통해 전달받은 모델의 파라미터를 update함\n",
    "\n",
    "- 모든 optimizer의 기본은 `torch.optim.Optimizer(params, defaults) 클래스를 사용함\n",
    "\n",
    "- `zero_grad()` 를 이용하여 optimizer가 update할 파라미터들의 기울기를 0으로 설정\n",
    "\n",
    "- `torch.optim.lr_scheduler`를 이용해 Epochs에 따라 learning rate를 조절할 수 있음\n",
    "\n",
    "- torch의 주요 optimizer\n",
    "    - `torch.optim.Adagrad()` : *Adaptive Subgradient Methods for Online Learning and Stochastic Optimization*\n",
    "\n",
    "    - `torch.optim.Adam()` : *Adam: A Method for Stochastic Optimization*\n",
    "\n",
    "    - `torch.optim.RMSprop()` : *G.Hinton's Lecture course*\n",
    "\n",
    "    - `torch.optim.SGD()` : *Stochastic gradient descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./images/Optimizer.gif>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate Scheduler\n",
    "- 학습 시 특정 조건에 따라 learning rate을 조정하면서 최적화를 수행\n",
    "- 일정 횟수 이상이 되면 learning rate decay(감소) / global minima 근처에 가면 learning rate 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch의 learning rate scheduler\n",
    "- `optim.lr_scheduler.LambdaLR()`\n",
    "    - 파이썬의 람다함수를 이용해 그 결과를 learning rate으로 설정\n",
    "\n",
    "    - lambda1 = lambda epoch: epoch // 30\n",
    "\n",
    "    - scheduler = LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "\n",
    "- `optim.lr_scheduler.StepLR()`\n",
    "    - step마다 learning rate을 gamma ratio 만큼 decay\n",
    "\n",
    "    - scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    - $epoch < 30 ⟶ lr = 0.01, 30 < epoch < 60 ⟶ lr = 0.001$ \n",
    "\n",
    "- `optim.lr_scheduler.MultiStepLR()`\n",
    "    - 위의 `StepLR()`과 비슷하지만 milestone을 지정해주어 특정 epoch에 gamma ratio 만큼 decay\n",
    "    \n",
    "    - scheduler = MultiStepLR(optimizer, milestones=[30, 50, 100], gamma=0.1)\n",
    "\n",
    "- `optim.lr_scheduler.ExponentialLR()`\n",
    "    - 매 epoch마다 learning rate * gamma 만큼 곱함\n",
    "\n",
    "    - 자주 사용되는 건 못 봄\n",
    "\n",
    "- `optim.lr_scheduler.CosineAnnealingLR()`\n",
    "    - *SGDR: Stochastic Gradient Descent with Warm Restarts*\n",
    "\n",
    "    - learning rate을 cosine 함수 형태처럼 변화시켜 학습률이 커질 때도 있고, 작아질 때도 있음\n",
    "\n",
    "- `optim.swa_utils`\n",
    "    - 2018, UAI *Averaging Weights Leads to Wider Optima and Better Generalizaion*\n",
    "\n",
    "    - 효과적이라서 torch 1.6 부터 공식적으로 지원해줌\n",
    "\n",
    "    - `from torch.optim.swa_utils import AverageModel, SWALR`\n",
    "\n",
    "    - `swa_model = AveragedModel(model)` - to compute the weights of the SWA model\n",
    "\n",
    "    - `swa_model.update_parameters(model)` - to update averages\n",
    "\n",
    "    - `swa_scheduler = SWALR(optimizer, anneal_strategy='Linear', ...)`\n",
    "\n",
    "    - `annel_strategy='cos'`로 하면 cosine annealing으로 사용할 수 있음\n",
    "\n",
    "    - 학습이 끝난 후에는 `swa_utils.update_bn()`으로 batch normalization statistics update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "- 모델의 학습과 테스트 단계를 monitoring 하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1615, 0.2764, 0.2743, 0.2155, 0.0723],\n",
      "        [0.7800, 0.0822, 0.0966, 0.0204, 0.0208],\n",
      "        [0.2098, 0.2646, 0.2503, 0.0135, 0.2617],\n",
      "        [0.1508, 0.2667, 0.2874, 0.1365, 0.1586],\n",
      "        [0.0954, 0.5569, 0.0468, 0.0116, 0.2893],\n",
      "        [0.0154, 0.2410, 0.3098, 0.3038, 0.1302],\n",
      "        [0.1011, 0.0948, 0.5678, 0.1392, 0.0971],\n",
      "        [0.4557, 0.0634, 0.0822, 0.1960, 0.2025],\n",
      "        [0.3925, 0.2493, 0.0603, 0.1380, 0.1599],\n",
      "        [0.1865, 0.3113, 0.2669, 0.0458, 0.1895]]) tensor([4, 0, 1, 2, 1, 0, 1, 0, 3, 1])\n",
      "tensor(0.6000)\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "preds = torch.randn(10, 5).softmax(dim=1)\n",
    "target = torch.randint(5, (10, ))\n",
    "print(preds, target)\n",
    "\n",
    "acc = torchmetrics.functional.accuracy(preds, target) # Function을 이용하여 평가\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4000)\n",
      "tensor(0.2000)\n",
      "tensor(0.3000)\n",
      "tensor(0.3000)\n",
      "tensor(0.)\n",
      "tensor(0.3000)\n",
      "tensor(0.4000)\n",
      "tensor(0.3000)\n",
      "tensor(0.3000)\n",
      "tensor(0.1000)\n",
      "tensor(0.2600)\n"
     ]
    }
   ],
   "source": [
    "metric = torchmetrics.Accuracy() # Module을 이용하여 평가\n",
    "n_batch = 10\n",
    "for i in range(n_batch):\n",
    "    preds = torch.randn(10, 5).softmax(dim=1)\n",
    "    target = torch.randint(5, (10, ))\n",
    "    acc = metric(preds, target) # 현재 batch의 accuracy 구하기\n",
    "    print(acc)\n",
    "\n",
    "acc = metric.compute() # 모든 batch의 accuracy 구하기\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch1.7.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a84903a2b3fdb7d367cfd9ea570be165ad361672f95a63c30b0740103c512e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
