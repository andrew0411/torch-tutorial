{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn ## Neural Network API\n",
    "import torch.nn.functional as F \n",
    "'''\n",
    "torch.nn은 nn.Module classes를 정의하지만 nn.functional은 말그대로 functional(stateless) 하게 접근한다\n",
    "다시 말해서 nn.Modules는 python class로 정의가 되고 따라서 attribute들이 존재한다\n",
    "ex. nn.Conv2d는 self.weight와 같은 internal attribute들이 있다\n",
    "\n",
    "반면에 F.conv2d는 그냥 operation이 정의되는 것이고 그냥 연산을 수행한다. internal attribute 없음\n",
    "그래서 stateless objesct에 forward 연산을 할 때는 functional API를 쓴다.\n",
    "ex. F.relu : nn.ReLU는 아무런 파라미터도 저장하지 않기때문에 굳이 모듈로 정의할 필요가 없음\n",
    "'''\n",
    "import torch.optim as optim ## Optimizer algorithm (SGD, Adam, Adagrad...)\n",
    "from torch.utils.data import DataLoader ## Dataset 관리, batch 생성을 도와주는 함수\n",
    "import torchvision.datasets as datasets ## MNIST, CIFAR10 등 benchmark dataset 관련\n",
    "import torchvision.transforms as transforms ## 이미지 데이터에 전처리하는 함수 모음 \n",
    "from torch.utils.tensorboard import SummaryWriter ## tensorboard에 출력하는 함수\n",
    "import torch.backends.cudnn as cudnn # cudnn 관련\n",
    "\n",
    "from torchsummary import summary ## model의 구조, 파라미터 수 등을 알 수 있게 해줌\n",
    "import torch.onnx ## torch model을 onnx로 변환하기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# cuda 사용 가능한지 \n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적으로 device 지정해줄 때 사용\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi GPU 사용할 때\n",
    "# 연구실 서버 기준 GPU 4개 중에서 0, 2, 3 가능하다면\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 2, 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 현재 사용가능한 GPU 개수 확인\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 일반적으로 multi GPU 사용할 때, GPU 할당\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(list(map(str, list(range(torch.cuda.device_count())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# cudnn 설정, GPU를 사용하고 있으면 cudnn.enabled = True\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "print(cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inbuild cudnn auto-tuner가 사용 중인 하드웨어에 가장 적합한 알고리즘을 선택할 수 있게 허용\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11553341440  byte\n",
      "11553.0  MB\n",
      "11.0  GB\n"
     ]
    }
   ],
   "source": [
    "# GPU device의 사용가능한 메모리 확인하기\n",
    "\n",
    "# byte 기준\n",
    "print(torch.cuda.get_device_properties('cuda:0').total_memory, ' byte')\n",
    "\n",
    "# mega byte 기준\n",
    "print(torch.cuda.get_device_properties('cuda:0').total_memory // 1e6, ' MB')\n",
    "\n",
    "# giga byte 기준\n",
    "print(torch.cuda.get_device_properties('cuda:0').total_memory // 1e9, ' GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi GPU 사용할 때 메모리 확인\n",
    "\n",
    "gpu_ids = list(map(str, list(range(torch.cuda.device_count()))))\n",
    "total_gpu_memory = 0\n",
    "for gpu_id in gpu_ids:\n",
    "    total_gpu_memory += torch.cuda.get_device_properties('cuda:'+gpu_id).total_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.cuda(non_blocking=True) 사용\n",
    "- 뒤에서 `torch.utils.data.DataLoader`의 `pin_memory`를 사용할 때 일반적으로 사용하는 옵션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, (images, target) in enumerate(train_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        images = images.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        target = target.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "    output = model(images)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 볼 수 있듯이 train_loader의 data를 GPU로 보내줄 때 `.cuda()`로 변환함\n",
    "\n",
    "이 때, `non_blocking=True` 를 옵션으로 지정해주게 되면 비동기식으로 GPU에 데이터를 전달할 수 있게 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('torch1.7.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a84903a2b3fdb7d367cfd9ea570be165ad361672f95a63c30b0740103c512e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
